# TrueLight - Intelligent Vision Assistant üö¶üëÅÔ∏è

> **Real-time object detection and audio alerts for color-blind and visually impaired users**

An accessibility-first mobile dashcam application that uses AI-powered computer vision to detect objects, analyze colors, and provide customized audio feedback for users with color vision deficiencies. Built with Expo, FastAPI, and YOLOv3.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Expo SDK 51+](https://img.shields.io/badge/expo-51+-000020.svg)](https://expo.dev/)
[![Next.js 15](https://img.shields.io/badge/next.js-15-black)](https://nextjs.org/)

---

## üìã Table of Contents

- [The Problem](#-the-problem)
- [The Solution](#-the-solution)
- [Key Features](#-key-features)
- [Tech Stack](#-tech-stack)
- [System Architecture](#-system-architecture)
- [Getting Started](#-getting-started)
- [Usage Guide](#-usage-guide)
- [How It Works](#-how-it-works)
- [Accessibility Features](#-accessibility-features)
- [API Reference](#-api-reference)
- [Troubleshooting](#-troubleshooting)
- [Contributing](#-contributing)
- [License](#-license)

---

## üö® The Problem

**300 million people worldwide** have color vision deficiency (colorblindness), making it difficult or impossible to distinguish between certain colors. This creates serious safety challenges:

- üö¶ **Traffic signals** rely on red/yellow/green colors
- üõë **Stop signs** and warning signs use specific colors
- üöó **Brake lights** indicate when vehicles are stopping
- üöß **Construction zones** use orange cones and signs
- üö® **Emergency vehicles** flash red and blue lights

For many people, these critical visual cues are ambiguous or invisible, creating dangerous situations during driving, biking, or walking.

---

## ‚úÖ The Solution

**TrueLight** transforms your smartphone into an intelligent vision assistant that:

1. üì∏ **Captures camera frames** in real-time
2. ü§ñ **Detects objects** using YOLOv3 deep learning model
3. üé® **Analyzes colors** with OpenCV HSV color detection
4. üó£Ô∏è **Announces alerts** via customized audio feedback
5. üìç **Shows bounding boxes** with animated targeting brackets
6. üö∂üö¥üöó **Adapts to transport mode** (walking, biking, driving)
7. üéØ **Prioritizes hazards** based on your specific colorblindness type

---

## üéØ Key Features

### Core Detection System
- ‚úÖ **Real-time Object Detection** - YOLOv3-tiny with 80 COCO classes
- ‚úÖ **Color Analysis Fallback** - Detects 7 color regions when YOLO finds nothing
- ‚úÖ **Always-On Detection** - Ensures something is always tracked
- ‚úÖ **Motion Tracking** - Follows moving objects across frames
- ‚úÖ **Confidence Thresholds** - Lowered to 10% for maximum recall

### Vision Customization
- ‚úÖ **Ishihara Color Vision Test** - 5-10 plate assessment
- ‚úÖ **Manual Type Selection** - Choose your colorblindness type anytime
- ‚úÖ **9 Vision Profiles Supported**:
  - Normal vision
  - Protanopia (red-blind)
  - Protanomaly (red-weak)
  - Deuteranopia (green-blind)
  - Deuteranomaly (green-weak)
  - Tritanopia (blue-blind)
  - Tritanomaly (blue-weak)
  - Achromatopsia (complete colorblindness)
  - Low vision / General visual impairment

### Visual Feedback
- ‚úÖ **Adaptive Color Palettes** - Never uses colors you can't see for alerts
- ‚úÖ **Animated Targeting Brackets** - Locks onto detected objects
- ‚úÖ **Color Labels** - Shows "RED/BLUE - car üöó" with transport context
- ‚úÖ **Flash Alerts** - Pulsing animation for problematic colors
- ‚úÖ **Active Target Indicator** - Highlights currently locked object

### Audio System
- ‚úÖ **Expo Speech (Primary)** - Works offline, instant feedback
- ‚úÖ **ElevenLabs TTS (Optional)** - Natural voice for enhanced experience
- ‚úÖ **Smart Debouncing** - Avoids repetitive alerts
- ‚úÖ **Adjustable Speech Rate** - 0.5x to 2.0x speed
- ‚úÖ **Position Cues** - "Top light is on" for traffic signals
- ‚úÖ **Proximity Alerts (Low Vision)** - "Warning! car very close ahead" based on object size
- ‚úÖ **Scene Description (Low Vision)** - Verbose audio description of top 3 objects on demand

### Transport Modes
- ‚úÖ **Walking Mode** üö∂ - 5s alerts, focuses on crosswalks/pedestrians
- ‚úÖ **Biking Mode** üö¥ - 3s alerts, prioritizes vehicles/bikes
- ‚úÖ **Driving Mode** üöó - 1.5s alerts, all traffic signals/signs
- ‚úÖ **Passenger Mode** üöó - 2s alerts, can relax without driving focus
- ‚úÖ **Low Vision Mode** üëÅÔ∏è - Urgency-based prioritization by object size/proximity instead of color
- ‚úÖ **Passenger Mode** üöå - Minimal alerts, emergency only
- ‚úÖ **Auto-Detection** - GPS speed-based mode switching

### AI Assistant "Sierra" (Optional)
- ‚úÖ **Voice Commands** - "Hey TrueLight" or "Sierra"
- ‚úÖ **Scene Analysis** - "What do you see?"
- ‚úÖ **Color Queries** - "What color is that?"
- ‚úÖ **Safety Checks** - "Can I cross?"
- ‚úÖ **Gemini 2.5 Flash** - Powered by Google AI

---

## üõ†Ô∏è Tech Stack

## üõ†Ô∏è Tech Stack

### Mobile App (Expo + React Native)
| Technology | Version | Purpose |
|------------|---------|---------|
| **Expo** | 51+ | React Native framework with managed workflow |
| **React Native** | Latest | Cross-platform mobile development |
| **TypeScript** | 5.x | Type-safe JavaScript |
| **Expo Camera** | Latest | Camera access and frame capture |
| **Expo Speech** | Latest | Text-to-speech audio alerts |
| **Expo Router** | Latest | File-based navigation |
| **Zustand** | 4.x | Lightweight state management |
| **AsyncStorage** | Latest | Local data persistence |

### Backend API (Next.js)
| Technology | Version | Purpose |
|------------|---------|---------|
| **Next.js** | 15 | API server and proxy |
| **TypeScript** | 5.x | Type safety |
| **App Router** | Latest | API route handling |
| **Node.js** | 18+ | Runtime environment |

### Detection Service (Python + FastAPI)
| Technology | Version | Purpose |
|------------|---------|---------|
| **Python** | 3.8+ | Backend language |
| **FastAPI** | 0.109+ | High-performance API framework |
| **Uvicorn** | 0.27+ | ASGI server |
| **OpenCV** | 4.9+ | Computer vision and color analysis |
| **NumPy** | 1.26+ | Array operations |
| **Pillow** | 10.2+ | Image processing |
| **YOLOv3-tiny** | - | Object detection model (~33MB) |

### Optional Services
| Service | Purpose | Required? |
|---------|---------|-----------|
| **Google Gemini 2.5 Flash** | AI voice assistant | No - voice commands disabled without it |
| **ElevenLabs** | Natural TTS voice | No - falls back to Expo Speech |
| **Roboflow** | Additional ML models | No - not currently used |

---

## üèóÔ∏è System Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      MOBILE APP (Expo)                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ  ‚îÇ Profile Tab  ‚îÇ  ‚îÇ Dashcam Tab  ‚îÇ  ‚îÇ Settings Tab ‚îÇ          ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ         ‚îÇ                 ‚îÇ                 ‚îÇ                   ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îÇ
‚îÇ                           ‚ñº                                     ‚îÇ
‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
‚îÇ         ‚îÇ    Zustand State Management         ‚îÇ                 ‚îÇ
‚îÇ         ‚îÇ  - Color Vision Profile             ‚îÇ                 ‚îÇ
‚îÇ         ‚îÇ  - Transport Mode                   ‚îÇ                 ‚îÇ
‚îÇ         ‚îÇ  - Alert Settings                   ‚îÇ                 ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îÇ                           ‚ñº                                     ‚îÇ
‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ
‚îÇ    ‚îÇ  Camera  ‚îÇ    ‚îÇ Location ‚îÇ    ‚îÇ  Audio   ‚îÇ               ‚îÇ
‚îÇ    ‚îÇ Component‚îÇ    ‚îÇ  Service ‚îÇ    ‚îÇ  Service ‚îÇ               ‚îÇ
‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ    API Request (Base64 Image)            ‚îÇ
        ‚îÇ    http://YOUR_IP:3000/api/detect        ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   NEXT.JS BACKEND (Port 3000)                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  API Routes (App Router)                                ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - /api/detect ‚Üí Proxy to Python service               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - /api/health ‚Üí Health check                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - /api/tts ‚Üí ElevenLabs integration (optional)        ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ    Forward to Python Detection Service    ‚îÇ
        ‚îÇ    http://localhost:8000/detect          ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ             PYTHON DETECTION SERVICE (Port 8000)                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  FastAPI Endpoints                                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - POST /detect ‚Üí Main detection endpoint              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - GET /health ‚Üí Service health check                  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - GET /test-detection ‚Üí Test with sample image        ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                           ‚ñº                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  Detection Pipeline                                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  1. Decode base64 image                                ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  2. Convert to numpy array                             ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  3. Run YOLO detection (confidence ‚â• 0.10)            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  4. If 0 detections ‚Üí Color region fallback            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  5. Analyze colors in HSV space                        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  6. Return top 5 detections with bounding boxes        ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                           ‚ñº                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
‚îÇ  ‚îÇ  YOLOv3-tiny  ‚îÇ         ‚îÇ  OpenCV Color   ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ  80 classes   ‚îÇ         ‚îÇ  Region Detect  ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ  640x640      ‚îÇ         ‚îÇ  HSV Analysis   ‚îÇ                ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Data Flow

1. **Camera Capture** (Mobile) ‚Üí Every 1.5-2s based on transport mode
2. **Image Encoding** ‚Üí Base64 JPEG at 70% quality
3. **API Request** ‚Üí Next.js backend (port 3000)
4. **Proxy** ‚Üí Python service (port 8000)
5. **YOLO Detection** ‚Üí YOLOv3-tiny processes image
6. **Color Fallback** ‚Üí If 0 detections, analyze color regions
7. **Response** ‚Üí JSON with bounding boxes, labels, colors, confidence
8. **Rendering** ‚Üí Animated brackets + audio alerts
9. **Audio Feedback** ‚Üí Expo Speech or ElevenLabs TTS

---

## üìÇ Project Structure

```
delta/
‚îú‚îÄ‚îÄ mobile/                          # Expo React Native App
‚îÇ   ‚îú‚îÄ‚îÄ app/                         # Expo Router screens
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ (tabs)/                  # Tab navigation
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ profile/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.tsx        # Profile overview
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test.tsx         # Color vision test
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ settings.tsx     # App settings
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ manual-select.tsx # Manual colorblind type picker
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dashcam/
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ index.tsx        # Camera/detection screen
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ home.tsx                 # Main dashboard
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ login.tsx                # Authentication
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ camera.tsx               # Standalone camera
‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ CameraView.tsx           # Camera capture logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ BoundingBoxOverlay.tsx   # Visual detection brackets
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SignalDisplay.tsx        # Traffic light UI
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ HazardOverlay.tsx        # Alert indicators
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api.ts                   # Backend API client
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MLService.ts             # Detection coordination
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ aiAssistant.ts           # Gemini voice commands
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AudioAlertService.ts     # TTS management
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ colorAnalyzer.ts         # Color processing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ speech.ts                # Speech synthesis
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ storage.ts               # Local persistence
‚îÇ   ‚îú‚îÄ‚îÄ store/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ useAppStore.ts           # Zustand state management
‚îÇ   ‚îú‚îÄ‚îÄ constants/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ accessibility.ts         # WCAG colors & types
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hazardPriority.ts        # Alert priorities
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ishihara.ts              # Color test data
‚îÇ   ‚îú‚îÄ‚îÄ assets/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logo.png                 # TrueLight logo
‚îÇ   ‚îú‚îÄ‚îÄ app.json                     # Expo configuration
‚îÇ   ‚îú‚îÄ‚îÄ package.json                 # Dependencies
‚îÇ   ‚îî‚îÄ‚îÄ tsconfig.json                # TypeScript config
‚îÇ
‚îú‚îÄ‚îÄ backend/                         # Next.js API Server
‚îÇ   ‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ api/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ detect/
‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ objects/
‚îÇ   ‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ route.ts     # Detection proxy endpoint
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ health/
‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ route.ts         # Health check
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ tts/
‚îÇ   ‚îÇ           ‚îî‚îÄ‚îÄ route.ts         # ElevenLabs TTS
‚îÇ   ‚îú‚îÄ‚îÄ lib/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ detection.ts             # Detection utilities
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ auth.ts                  # JWT authentication
‚îÇ   ‚îú‚îÄ‚îÄ .env                         # Environment variables
‚îÇ   ‚îú‚îÄ‚îÄ next.config.js               # Next.js config
‚îÇ   ‚îú‚îÄ‚îÄ package.json                 # Dependencies
‚îÇ   ‚îî‚îÄ‚îÄ tsconfig.json                # TypeScript config
‚îÇ
‚îú‚îÄ‚îÄ python-detection/                # Python Detection Service
‚îÇ   ‚îú‚îÄ‚îÄ main.py                      # FastAPI server
‚îÇ   ‚îú‚îÄ‚îÄ detector.py                  # YOLO + OpenCV detection
‚îÇ   ‚îú‚îÄ‚îÄ color_analyzer.py            # HSV color analysis
‚îÇ   ‚îú‚îÄ‚îÄ models/                      # YOLOv3 model files
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ yolov3-tiny.weights      # ~33MB
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ yolov3-tiny.cfg          # Config
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ coco.names               # 80 class names
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt             # Python dependencies
‚îÇ   ‚îú‚îÄ‚îÄ setup.bat                    # Windows setup script
‚îÇ   ‚îî‚îÄ‚îÄ start.bat                    # Windows start script
‚îÇ
‚îú‚îÄ‚îÄ README.md                        # This file
‚îî‚îÄ‚îÄ FEATURES_ROADMAP.md              # Product roadmap

```

---

## üöÄ Getting Started

### Prerequisites

Ensure you have the following installed:

- ‚úÖ **Node.js 18+** and npm - [Download](https://nodejs.org/)
- ‚úÖ **Python 3.8+** and pip - [Download](https://www.python.org/downloads/)
- ‚úÖ **Expo Go** on your phone - [iOS](https://apps.apple.com/app/expo-go/id982107779) | [Android](https://play.google.com/store/apps/details?id=host.exp.exponent)
- ‚úÖ **Git** - [Download](https://git-scm.com/)
- ‚ö†Ô∏è Computer and phone on **same Wi-Fi network**

### Quick Start (5 minutes)

#### 1Ô∏è‚É£ Clone the Repository

```bash
git clone https://github.com/yourusername/truelight.git
cd truelight
```

#### 2Ô∏è‚É£ Setup Python Detection Service

```bash
cd python-detection

# Install dependencies
pip install -r requirements.txt

# Download YOLO model (~33MB) - auto-downloads on first run
# Or manually: https://pjreddie.com/media/files/yolov3-tiny.weights
# Place in: python-detection/models/yolov3-tiny.weights

# Start service
python main.py
```

**Expected output:**
```
INFO:     Uvicorn running on http://0.0.0.0:8000
INFO:     YOLOv3-tiny model loaded successfully
INFO:     Loaded 80 COCO classes
```

**Verify:** Open browser to `http://localhost:8000/health` ‚Üí Should see `{"status": "healthy", "yolo_loaded": true}`

#### 3Ô∏è‚É£ Setup Next.js Backend

```bash
cd backend

# Install dependencies
npm install

# Create .env file (optional)
```bash
# Create .env file (optional)
# Defaults work for local development
echo "PYTHON_DETECTION_URL=http://localhost:8000" > .env

# Start backend
npm run dev
```

**Expected output:**
```
‚ñ≤ Next.js 15.0.0
- Local:        http://localhost:3000
- Ready in 2.3s
```

**Verify:** `curl http://localhost:3000/api/health` ‚Üí Should see `{"status": "ok", ...}`

#### 4Ô∏è‚É£ Setup Mobile App

```bash
cd mobile

# Install dependencies
npm install

# Find your computer's IP address
# Windows:
ipconfig  # Look for IPv4 Address under Wi-Fi (e.g., 192.168.1.100)

# macOS/Linux:
ifconfig | grep "inet " | grep -v 127.0.0.1

# Create .env file with YOUR IP
echo "EXPO_PUBLIC_API_URL=http://YOUR_IP_HERE:3000" > .env
# Example: echo "EXPO_PUBLIC_API_URL=http://192.168.1.100:3000" > .env

# Optional: Add Gemini API key for voice commands
echo "EXPO_PUBLIC_GEMINI_API_KEY=your_key_here" >> .env

# Start Expo
npx expo start --clear
```

**Expected output:**
```
Metro waiting on exp://192.168.1.100:8081
‚Ä∫ Scan the QR code above with Expo Go (Android) or the Camera app (iOS)
```

#### 5Ô∏è‚É£ Run on Your Phone

1. Open **Expo Go** app on your phone
2. Scan the QR code from the terminal
3. App will load (first time takes ~30 seconds)
4. Grant **camera** and **microphone** permissions
5. Complete vision profile setup (or skip)
6. Tap "START DASHCAM"

---

### Environment Variables Reference

#### Backend `.env` (Optional)

All have sensible defaults for local development:

```bash
# Python service URL (default: http://localhost:8000)
PYTHON_DETECTION_URL=http://localhost:8000

# Optional: ElevenLabs TTS (falls back to Expo Speech)
ELEVENLABS_API_KEY=sk_xxxxx

# Optional: Roboflow API (not currently used)
ROBOFLOW_API_KEY=xxxxx

# JWT secret (change in production)
JWT_SECRET=your-secret-key-here
```

#### Mobile `.env` (Required)

```bash
# REQUIRED: Your computer's local IP + port 3000
EXPO_PUBLIC_API_URL=http://192.168.1.100:3000

# OPTIONAL: Google Gemini for voice commands
EXPO_PUBLIC_GEMINI_API_KEY=AIzaSyXXXXXX
```

**‚ö†Ô∏è Important:** 
- Use your computer's **local IP address** (not `localhost`)
- Find it with `ipconfig` (Windows) or `ifconfig` (Mac/Linux)
- Example: `192.168.1.100`, `10.0.0.5`, `172.16.0.10`
- Phone and computer must be on same Wi-Fi network

---

## üì± Usage Guide

### First Launch

1. **Vision Profile Setup** (Optional)
   - Take 5-plate color vision test (~30 seconds)
   - Or manually select your colorblindness type
   - Or skip and use normal vision profile

2. **Grant Permissions**
   - Allow camera access for object detection
   - Allow microphone for voice commands (optional)

3. **Choose Transport Mode**
   - Settings ‚Üí Transport Mode
   - Walking üö∂ / Biking üö¥ / Driving üöó / Passenger üöå
   - Or enable auto-detection via GPS speed

### Using the Dashcam

1. Tap "**START DASHCAM**" from home screen
2. Point camera at objects/traffic signals
3. Bounding boxes appear in real-time
4. Audio alerts announce detected hazards
5. Tap objects to lock focus

**What You'll See:**
- üéØ **Animated brackets** around detected objects
- üè∑Ô∏è **Color labels**: "RED/WHITE - car üöó"
- ‚ö†Ô∏è **Flash alerts** for problematic colors
- üìä **Confidence scores** on each detection
- üîä **Audio announcements** for hazards

### Voice Commands (Optional - Requires Gemini API)

Say "**Hey TrueLight**" or "**Sierra**" followed by:

| Command | Response |
|---------|----------|
| "What do you see?" | Detailed scene description |
| "What color is that?" | Identifies colors in view |
| "Can I cross?" | Checks if it's safe to proceed |
| "What's ahead?" | Describes upcoming hazards |
| "Help" | Lists available commands |

### Customization Settings

**Profile ‚Üí Settings:**

- **Alert Level**: Minimal / Standard / Verbose
- **Transport Mode**: Walking / Biking / Driving / Passenger
- **Speech Rate**: 0.5x to 2.0x speed
- **Position Cues**: Enable "top light" announcements
- **Shape Indicators**: Add shapes to UI
- **Voice Provider**: System TTS or ElevenLabs
- **Detection Types**: Toggle which objects to detect

**Change Vision Type Anytime:**
- Profile ‚Üí Settings ‚Üí Color Vision Type ‚Üí Change Vision Type
- Select from 9 types without retaking test

---

## üîß How It Works

### Detection Pipeline

```
Camera Frame (720x1280)
     ‚Üì
[Capture every 1.5-2s based on transport mode]
     ‚Üì
[Convert to JPEG at 70% quality]
     ‚Üì
[Encode to base64 string]
     ‚Üì
[Send to Next.js backend at http://YOUR_IP:3000/api/detect]
     ‚Üì
[Proxy to Python service at http://localhost:8000/detect]
     ‚Üì
[Decode base64 ‚Üí NumPy array]
     ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ YOLO Detection (confidence ‚â• 10%) ‚îÇ
‚îÇ - 80 COCO classes                ‚îÇ
‚îÇ - 640x640 input size             ‚îÇ
‚îÇ - NMS threshold: 0.4             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚Üì
[Check detection count]
     ‚Üì
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ > 0 ‚îÇ YES ‚Üí Return YOLO detections
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚Üì NO
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Color Region Fallback            ‚îÇ
‚îÇ - Convert to HSV color space    ‚îÇ
‚îÇ - Detect 7 color regions:       ‚îÇ
‚îÇ   Red, Orange, Yellow, Green,   ‚îÇ
‚îÇ   Blue, Purple, Pink            ‚îÇ
‚îÇ - Find contours ‚â• 1500px¬≤      ‚îÇ
‚îÇ - Return top 5 by area          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚Üì
[Return JSON response]
     ‚Üì
{
  "success": true,
  "detections": [
    {
      "label": "car",
      "confidence": 0.78,
      "bbox": {"x": 120, "y": 300, "width": 180, "height": 150},
      "colors": ["red", "white"]
    }
  ]
}
     ‚Üì
[Mobile renders bounding boxes]
     ‚Üì
[Audio alert if needed]
```

### Color Analysis

Objects are analyzed in **HSV color space** for robustness:

| Color | H Range | S Range | V Range |
|-------|---------|---------|---------|
| Red | 0-10, 170-180 | 100-255 | 100-255 |
| Orange | 10-25 | 100-255 | 100-255 |
| Yellow | 25-35 | 100-255 | 100-255 |
| Green | 35-85 | 50-255 | 50-255 |
| Blue | 85-130 | 50-255 | 50-255 |
| Purple | 130-160 | 50-255 | 50-255 |
| Pink | 160-170 | 50-255 | 50-255 |

**Why HSV?**
- More robust to lighting changes than RGB
- Easier to define color ranges
- Better for outdoor/varying conditions

### Adaptive Color System

TrueLight **never uses colors you can't see** for alerts:

| Colorblindness Type | Standard Alert | TrueLight Alert |
|---------------------|----------------|-----------------|
| Protanopia (red-blind) | ‚ùå Red | ‚úÖ Cyan |
| Deuteranopia (green-blind) | ‚ùå Green | ‚úÖ Pink |
| Tritanopia (blue-blind) | ‚ùå Blue | ‚úÖ Orange-Red |
| Normal vision | ‚úÖ Red/Green | ‚úÖ Red/Green |

### Transport Mode Adaptation

Frame processing and alert intervals adjust to your speed:

| Mode | Speed Range | Frame Interval | Alert Interval | Priority |
|------|-------------|----------------|----------------|----------|
| üö∂ Walking | 0-5 km/h | 250ms | 5000ms | Crosswalks, pedestrians |
| üö¥ Biking | 5-25 km/h | 200ms | 3000ms | Vehicles, bike lanes |
| üöó Driving | 25-80 km/h | 125ms | 1500ms | All traffic signals |
| üöå Passenger | Any | 250ms | 10000ms | Emergency only |

---

## ‚ôø Accessibility Features

### Visual Accessibility
‚úÖ **WCAG AAA** color contrast ratios  
‚úÖ **Dark mode** default to reduce eye strain  
‚úÖ **Large touch targets** (minimum 48dp)  
‚úÖ **High contrast** UI elements  
‚úÖ **Shape indicators** alongside colors (‚ñ† ‚óè ‚ñ≤)  
‚úÖ **Animated brackets** for better visibility  

### Colorblindness Support
‚úÖ **9 vision profiles** supported  
‚úÖ **Adaptive color palettes** per type  
‚úÖ **Never uses invisible colors** for alerts  
‚úÖ **Position cues**: "Top light is on"  
‚úÖ **Manual type selection** anytime  
‚úÖ **Color name labels** on all detections  

### Audio Accessibility
‚úÖ **Offline TTS** (Expo Speech)  
‚úÖ **Optional natural voice** (ElevenLabs)  
‚úÖ **Adjustable speech rate** (0.5x - 2.0x)  
‚úÖ **Smart debouncing** (no repetitive alerts)  
‚úÖ **Context-aware messages** per transport mode  
‚úÖ **Screen reader compatible**  
‚úÖ **Proximity alerts** (low vision users) - Urgency-based voice alerts for close objects  
‚úÖ **Scene description** (low vision users) - On-demand verbal description of surroundings  

### Low Vision Mode Enhancements
**TrueLight's low vision mode prioritizes by urgency and proximity rather than color:**

üîä **Proximity-Based Voice Alerts:**
- Objects >10% of frame = "Warning! [object] very close [direction]" (fast, urgent voice)
- Objects >5% of frame = "[object] approaching [direction]" (moderate urgency)
- Objects >2% of frame = "[object] ahead" (normal pace)
- Direction cues: "left", "right", "ahead"

üì¢ **Scene Description On Demand:**
- Tap "Describe Scene" button to hear top 3 objects
- Example: "3 objects detected. Large car ahead. Medium person left. Small sign right."
- Helps users understand their full surroundings

üéØ **Urgency-Based Prioritization:**
- Python backend calculates object size relative to frame
- Large objects = critical priority (regardless of color)
- Moving + large = highest priority
- Visual overlay sorts by size, not color
- Audio alerts focus on closest threats first  

### Cognitive Accessibility
‚úÖ **Simple UI** with minimal distractions  
‚úÖ **Clear iconography**  
‚úÖ **Consistent navigation**  
‚úÖ **Confirmation dialogs** for critical actions  
‚úÖ **Progressive disclosure** of settings  

---

## üì° API Reference

### Python Detection Service

**Base URL:** `http://localhost:8000`

#### POST `/detect`

Detect objects and colors in an image.

**Request:**
```json
{
  "image": "base64_encoded_jpeg_string",
  "confidence_threshold": 0.10,
  "nms_threshold": 0.4
}
```

**Response:**
```json
{
  "success": true,
  "num_detections": 2,
  "detections": [
    {
      "label": "car",
      "confidence": 0.78,
      "bbox": {
        "x": 120,
        "y": 300,
        "width": 180,
        "height": 150
      },
      "colors": ["red", "white"],
      "class_id": 2
    },
    {
      "label": "traffic light",
      "confidence": 0.65,
      "bbox": {
        "x": 450,
        "y": 50,
        "width": 40,
        "height": 120
      },
      "colors": ["red", "yellow"],
      "class_id": 9
    }
  ],
  "image_size": "640x480",
  "processing_time_ms": 156
}
```

#### GET `/health`

Check service health and model status.

**Response:**
```json
{
  "status": "healthy",
  "yolo_loaded": true,
  "model_classes": 80,
  "version": "1.0.0"
}
```

#### GET `/test-detection`

Test detection with a sample image.

**Response:**
```json
{
  "success": true,
  "image_size": "640x452",
  "num_detections": 1,
  "detections": [...]
}
```

### Next.js Backend

**Base URL:** `http://localhost:3000`

#### POST `/api/detect/objects`

Proxy to Python detection service.

**Request:**
```json
{
  "image": "base64_string",
  "confidence": 0.15
}
```

**Response:** Same as Python `/detect` endpoint

#### GET `/api/health`

Backend health check.

**Response:**
```json
{
  "status": "ok",
  "timestamp": "2026-01-11T12:00:00Z",
  "python_service": "connected"
}
```

---

## üêõ Troubleshooting

### Common Issues

<details>
<summary><b>üî¥ Python Service Won't Start</b></summary>

**Problem:** `ModuleNotFoundError: No module named 'fastapi'`

**Solution:**
```bash
cd python-detection
pip install -r requirements.txt
```

**Problem:** `YOLO model not loaded`

**Solution:**
```bash
# Download YOLOv3-tiny weights manually
cd python-detection/models
# Download from: https://pjreddie.com/media/files/yolov3-tiny.weights
# Also need: https://github.com/pjreddie/darknet/blob/master/cfg/yolov3-tiny.cfg
```

**Problem:** Port 8000 already in use

**Solution:**
```bash
# Windows
netstat -ano | findstr :8000
taskkill /PID <PID> /F

# macOS/Linux
lsof -ti:8000 | xargs kill -9
```

</details>

<details>
<summary><b>üü° Backend Won't Start</b></summary>

**Problem:** Port 3000 already in use

**Solution:**
```bash
# Windows
netstat -ano | findstr :3000
taskkill /PID <PID> /F

# macOS/Linux
lsof -ti:3000 | xargs kill -9
```

**Problem:** Can't connect to Python service

**Solution:**
1. Verify Python service is running: `curl http://localhost:8000/health`
2. Check `backend/.env` has `PYTHON_DETECTION_URL=http://localhost:8000`
3. Restart both services

</details>

<details>
<summary><b>üü¢ Mobile App Issues</b></summary>

**Problem:** "Network request failed"

**Solutions:**
1. ‚úÖ Verify phone and computer on **same Wi-Fi** (not mobile data!)
2. ‚úÖ Check `mobile/.env` has your **local IP** (not `localhost`)
3. ‚úÖ Disable VPN on computer and phone
4. ‚úÖ Check firewall allows port 3000
5. ‚úÖ Test: `curl http://YOUR_IP:3000/api/health` from another device

**Problem:** No detections showing

**Solutions:**
1. Check Python service logs for errors
2. Ensure camera permissions granted
3. Point at well-lit objects
4. Check mobile console: `npx expo start` ‚Üí press `j` for debugger
5. Verify services: `curl http://localhost:8000/health`

**Problem:** No audio alerts

**Solutions:**
1. Check device volume
2. Ensure microphone permission granted (needed for voice commands)
3. Settings ‚Üí Voice Provider ‚Üí Try switching providers
4. Test: Settings ‚Üí Test Voice button

**Problem:** Bounding boxes not appearing

**Solutions:**
1. Reload app: Press `r` in Expo terminal
2. Check detection confidence: Settings ‚Üí lower thresholds
3. Point at larger, well-lit objects
4. Transport mode affects frequency: Settings ‚Üí Transport Mode

</details>

<details>
<summary><b>üîµ General Issues</b></summary>

**Problem:** App crashes on load

**Solution:**
```bash
cd mobile
npx expo start --clear  # Clear cache
# On phone: Delete Expo app data and reinstall
```

**Problem:** Slow detection

**Solution:**
1. Lower image quality: CameraView.tsx ‚Üí quality: 0.5
2. Increase frame interval: Settings ‚Üí Transport Mode
3. Check Python service isn't overloaded: `top` or Task Manager

**Problem:** Changes not appearing

**Solution:**
```bash
# In Expo terminal, press:
r  # Reload app
c  # Clear cache and restart

# Or full reset:
npx expo start --clear
```

</details>

---

## üî¨ Testing

### Manual Testing

```bash
# Test Python service
curl -X GET http://localhost:8000/health
curl -X GET http://localhost:8000/test-detection

# Test backend
curl -X GET http://localhost:3000/api/health

# Test full detection (from mobile logs)
# Check Expo console for detection responses
```

### Unit Tests (Coming Soon)

```bash
# Python tests
cd python-detection
pytest tests/

# Backend tests
cd backend
npm test

# Mobile tests
cd mobile
npm test
```

---

## ü§ù Contributing

We welcome contributions! Here's how:

1. **Fork** the repository
2. **Create** a feature branch: `git checkout -b feature/amazing-feature`
3. **Commit** changes: `git commit -m 'Add amazing feature'`
4. **Push** to branch: `git push origin feature/amazing-feature`
5. **Open** a Pull Request

### Development Guidelines

- **Code Style**: Follow existing TypeScript/Python conventions
- **Comments**: Document complex logic
- **Types**: Use TypeScript types, Python type hints
- **Accessibility**: Maintain WCAG AAA compliance
- **Testing**: Add tests for new features
- **Commits**: Use conventional commit messages

---

## üìÑ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgments

- **YOLOv3** - Joseph Redmon, Ali Farhadi ([Paper](https://arxiv.org/abs/1804.02767))
- **OpenCV** - Computer vision library
- **Expo** - React Native development platform
- **FastAPI** - High-performance Python web framework
- **Color Blind Awareness** - Color vision deficiency research
- **Ishihara Test** - Color vision testing methodology

---

## üìû Support

- **Issues**: [GitHub Issues](https://github.com/yourusername/truelight/issues)
- **Discussions**: [GitHub Discussions](https://github.com/yourusername/truelight/discussions)
- **Email**: support@truelight.app

---

## üó∫Ô∏è Roadmap

See [FEATURES_ROADMAP.md](FEATURES_ROADMAP.md) for detailed feature plans.

### Upcoming Features

- [ ] **Continuous Recording** - Loop recording with incident clips
- [ ] **Cloud Sync** - Backup settings across devices
- [ ] **Brake Light Detection** - Real-time vehicle braking alerts
- [ ] **Stop Sign Detection** - Octagonal sign recognition
- [ ] **Emergency Vehicle Detection** - Flashing light patterns
- [ ] **Apple Watch Integration** - Haptic alerts
- [ ] **Offline Mode** - Full functionality without internet
- [ ] **Multi-language Support** - Spanish, French, Mandarin, etc.

---

<div align="center">

**Built with ‚ù§Ô∏è for accessibility**

**TrueLight** - See the world clearly

[Report Bug](https://github.com/yourusername/truelight/issues) ¬∑ [Request Feature](https://github.com/yourusername/truelight/issues) ¬∑ [Documentation](https://github.com/yourusername/truelight/wiki)

</div>
